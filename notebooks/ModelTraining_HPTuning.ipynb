{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951af4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.model.model import SpeakerCountCNN\n",
    "import src.model.model as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49babb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_dir):\n",
    "\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = self.data['speaker_count'].astype(int).tolist()  # <- add this\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "    \n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        row = self.data.iloc[idx]\n",
    "        tensor_path = os.path.join(self.data_dir, row['spectrogram'])\n",
    "        spectrogram = torch.load(tensor_path).unsqueeze(0).float();  # shape: [1, H, W]\n",
    "        label = int(row['speaker_count'])\n",
    "        return spectrogram, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc3e60",
   "metadata": {},
   "source": [
    "### Initial Model\n",
    "~82% accuracy on val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7507bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creates torch dataset using spectrogram files as 'x' and csv of labels as 'y'\n",
    "# dataset = SpectrogramDataset(csv_file=r\"data/spectrogram_labels.csv\", data_dir=r\"data/spectrograms\") # grab dataset and convert to tensor\n",
    "\n",
    "\n",
    "\n",
    "# # Hyperparams\n",
    "# conv1_out = 16\n",
    "# conv2_out = 32\n",
    "# conv3_out = 64\n",
    "# dropout_prob = 0.3\n",
    "# fc_hidden = 128\n",
    "# num_classes = 4\n",
    "# # fixed image size\n",
    "# input_height = 96\n",
    "# input_width = 64\n",
    "\n",
    "\n",
    "\n",
    "# # init model\n",
    "# model = SpeakerCountCNN(\n",
    "#     input_height = input_height,\n",
    "#     input_width = input_width,\n",
    "#     conv1_out = conv1_out,\n",
    "#     conv2_out = conv2_out,\n",
    "#     conv3_out = conv3_out,\n",
    "#     fc_hidden = fc_hidden,\n",
    "#     dropout_prob = dropout_prob\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # manual train/test/split\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "# # batch loader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# # cpu or gpu; whichever is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# # basic loss and optimiser\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "# # Model training\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     start_time = time()\n",
    "\n",
    "#     # tqdm shows a progress bar\n",
    "#     for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(x)\n",
    "#         loss = criterion(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     end_time = time()\n",
    "#     epoch_duration = end_time - start_time\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} — Loss: {avg_loss:.4f} — Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "\n",
    "# # Validation - this is the important metric\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for x, y in val_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         output = model(x)\n",
    "#         preds = output.argmax(dim=1)\n",
    "#         correct += (preds == y).sum().item()\n",
    "#         total += y.size(0)\n",
    "\n",
    "# print(f\"Validation Accuracy: {correct / total:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "# # save\n",
    "# # torch.save(model.state_dict(), 'SpeakerCountCNN_v0.01.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b819e02",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Starting with random search to find the 'general area' for the best params. Then a grid-search to really refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b747e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 1: stratified sampling. Want to use a subset of the data for quick tuning, but can't have class imbalance. Solution is sss\n",
    "# from torch.utils.data import Subset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# # Get labels\n",
    "# full_dataset = dataset # pointer fun\n",
    "# labels = dataset.labels\n",
    "\n",
    "\n",
    "\n",
    "# # Sample 30% with stratification - a cheeky way to use train/test split\n",
    "# subset_indices, _ = train_test_split(\n",
    "#     list(range(len(full_dataset))),\n",
    "#     train_size=0.3,\n",
    "#     stratify=labels,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Create a reduced dataset\n",
    "# reduced_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "\n",
    "\n",
    "# # Now split reduced dataset into train/val (e.g. 80/20)\n",
    "# reduced_labels = [dataset.labels[i] for i in subset_indices]\n",
    "# train_idx, val_idx = train_test_split(\n",
    "#     list(range(len(reduced_dataset))),\n",
    "#     train_size=0.8,\n",
    "#     stratify=reduced_labels,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(Subset(reduced_dataset, train_idx), batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(Subset(reduced_dataset, val_idx), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9007652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparam search space for Random Search (RS)\n",
    "# # RS works by randomly selecting a config, num_trials times. This gives a general idea of a good config without iterating through every possible config\n",
    "# param_space = {\n",
    "#     \"lr\": [1e-4, 1e-3, 1e-2],\n",
    "#     \"dropout\": [0.1, 0.3, 0.5],\n",
    "#     \"fc_hidden\": [64, 128, 256],\n",
    "#     \"conv1_out\": [8, 16, 32],\n",
    "#     \"conv2_out\": [16, 32, 64],\n",
    "#     \"conv3_out\": [32, 64, 128],\n",
    "#     \"optimizer\": [\"Adam\", \"SGD\"]\n",
    "# }\n",
    "\n",
    "# # RS is very simple to implement, one line of code\n",
    "# def sample_config():\n",
    "#     return {k: random.choice(v) for k, v in param_space.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ceca7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10 configs pulled randomly from param_space, each trained for 3 epochs\n",
    "# num_trials = 10\n",
    "# epochs = 3\n",
    "# results = []\n",
    "\n",
    "# for trial in range(1, num_trials + 1):\n",
    "#     config = sample_config()\n",
    "#     print(f\"\\n=== Trial {trial}/{num_trials} ===\")\n",
    "#     print(f\"Config: {config}\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Setting up the model with the current random config\n",
    "#     model = SpeakerCountCNN(\n",
    "#         input_height=input_height,\n",
    "#         input_width=input_width,\n",
    "#         conv1_out=config[\"conv1_out\"],\n",
    "#         conv2_out=config[\"conv2_out\"],\n",
    "#         conv3_out=config[\"conv3_out\"],\n",
    "#         fc_hidden=config[\"fc_hidden\"],\n",
    "#         dropout_prob=config[\"dropout\"]\n",
    "#     ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#     # Two optimisers are tested in the random search\n",
    "#     if config[\"optimizer\"] == \"Adam\":\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "#     else:\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "\n",
    "\n",
    "#     # Cross entropy loss\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "#     # Training\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         start = time()\n",
    "\n",
    "\n",
    "#         for xb, yb in tqdm(train_loader, desc=f\"Trial {trial} — Epoch {epoch}\", leave=False):\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             out = model(xb)\n",
    "#             loss = criterion(out, yb)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "\n",
    "#         # This is just verbose to inspect progress\n",
    "#         duration = time() - start\n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "#         print(f\"Epoch {epoch} — Loss: {avg_loss:.4f} — Time: {duration:.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             preds = model(xb).argmax(dim=1)\n",
    "#             correct += (preds == yb).sum().item()\n",
    "#             total += yb.size(0)\n",
    "\n",
    "#     acc = correct / total\n",
    "#     print(f\"Validation Accuracy: {acc:.2%}\")\n",
    "#     results.append((acc, config))\n",
    "\n",
    "\n",
    "\n",
    "# # Show top results\n",
    "# results.sort(reverse=True, key=lambda x: x[0])\n",
    "# print(\"\\n=== Top Results ===\")\n",
    "# for acc, cfg in results[:5]:\n",
    "#     print(f\"Accuracy: {acc:.4f} | Config: {cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a4cb6",
   "metadata": {},
   "source": [
    "### Random Search complete - now Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf8b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid search HPs manually interpretted from random search results\n",
    "# param_grid = {\n",
    "#     \"lr\":       [5e-4, 1e-3, 5e-3],\n",
    "#     \"dropout\":  [0.25, 0.3, 0.35],\n",
    "#     \"fc_hidden\":[64, 128, 256],\n",
    "#     \"conv1_out\":[16, 32],\n",
    "#     \"conv2_out\":[16, 32, 64],\n",
    "#     \"conv3_out\":[64, 128],\n",
    "#     \"optimizer\":[\"Adam\"],\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# # Utility to expand the grid into a list of configs\n",
    "# from itertools import product\n",
    "# def generate_grid_configs(grid):\n",
    "#     \"\"\"\n",
    "#     Yields every combination of hyperparameters in `grid`.\n",
    "#     \"\"\"\n",
    "#     keys = list(grid.keys())\n",
    "#     for vals in product(*grid.values()):\n",
    "#         yield dict(zip(keys, vals))\n",
    "\n",
    "\n",
    "\n",
    "# # Build and inspect\n",
    "# grid_configs = list(generate_grid_configs(param_grid))\n",
    "# print(f\"Total configurations: {len(grid_configs)}\") \n",
    "\n",
    "\n",
    "\n",
    "# train_loader, val_loader = get_stratified_loaders(\n",
    "#     dataset,\n",
    "#     subset_frac=0.1,   \n",
    "#     train_frac=0.8,    \n",
    "#     batch_size=32,\n",
    "#     random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b32fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "# epochs = 4\n",
    "\n",
    "\n",
    "# # For each config\n",
    "# for idx, config in enumerate(grid_configs, start=1):\n",
    "#     print(f\"\\n=== Config {idx}/{len(grid_configs)} ===\")\n",
    "#     print(f\"Config: {config}\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Build model\n",
    "#     model = SpeakerCountCNN(\n",
    "#         input_height=input_height,\n",
    "#         input_width=input_width,\n",
    "#         conv1_out=config[\"conv1_out\"],\n",
    "#         conv2_out=config[\"conv2_out\"],\n",
    "#         conv3_out=config[\"conv3_out\"],\n",
    "#         fc_hidden=config[\"fc_hidden\"],\n",
    "#         dropout_prob=config[\"dropout\"]\n",
    "#     ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#     # Adam optim\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "\n",
    "\n",
    "#     # Cross Entropy loss\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         start = time()\n",
    "\n",
    "#         for xb, yb in tqdm(train_loader, desc=f\"Conf {idx} — Ep {epoch}\", leave=False):\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             out = model(xb)\n",
    "#             loss = criterion(out, yb)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         duration = time() - start\n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "#         print(f\"Epoch {epoch} — Loss: {avg_loss:.4f} — Time: {duration:.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             preds = model(xb).argmax(dim=1)\n",
    "#             correct += (preds == yb).sum().item()\n",
    "#             total += yb.size(0)\n",
    "\n",
    "#     acc = correct / total\n",
    "#     print(f\"Validation Accuracy: {acc:.2%}\")\n",
    "#     results.append((acc, config))\n",
    "\n",
    "\n",
    "\n",
    "# # 4. Summarize top performers\n",
    "# results.sort(reverse=True, key=lambda x: x[0])\n",
    "# print(\"\\n=== Top Results ===\")\n",
    "# for acc, cfg in results[:5]:\n",
    "#     print(f\"Accuracy: {acc:.4f} | Config: {cfg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b087056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# # --- Helper functions ---\n",
    "# def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for xb, yb in loader:\n",
    "#         xb, yb = xb.to(device), yb.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(xb)\n",
    "#         loss = criterion(out, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "# def validate(model, loader, device):\n",
    "#     model.eval()\n",
    "#     correct, total = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             preds = model(xb).argmax(dim=1)\n",
    "#             correct += (preds == yb).sum().item()\n",
    "#             total += yb.size(0)\n",
    "#     return correct / total\n",
    "\n",
    "# # --- Objective for Optuna ---\n",
    "# def objective(trial):\n",
    "#     # 1) Sample hyperparameters\n",
    "#     lr        = trial.suggest_categorical(\"lr\", [5e-4, 1e-3, 5e-3])\n",
    "#     dropout   = trial.suggest_categorical(\"dropout\", [0.25, 0.3, 0.35])\n",
    "#     fc_hidden = trial.suggest_categorical(\"fc_hidden\", [64, 128, 256])\n",
    "#     conv1_out = trial.suggest_categorical(\"conv1_out\", [16, 32])\n",
    "#     conv2_out = trial.suggest_categorical(\"conv2_out\", [16, 32, 64])\n",
    "#     conv3_out = trial.suggest_categorical(\"conv3_out\", [64, 128])\n",
    "\n",
    "#     # 2) Build model\n",
    "#     model = SpeakerCountCNN(\n",
    "#         input_height=input_height,\n",
    "#         input_width=input_width,\n",
    "#         conv1_out=conv1_out,\n",
    "#         conv2_out=conv2_out,\n",
    "#         conv3_out=conv3_out,\n",
    "#         fc_hidden=fc_hidden,\n",
    "#         dropout_prob=dropout\n",
    "#     ).to(device)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # 3) Training + pruning\n",
    "#     max_epochs = 4\n",
    "#     for epoch in range(max_epochs):\n",
    "#         train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "#         val_acc    = validate(model, val_loader, device)\n",
    "\n",
    "#         # report intermediate objective value\n",
    "#         trial.report(val_acc, epoch)\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "#     return val_acc  # final validation accuracy\n",
    "\n",
    "# # --- Run study ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     study = optuna.create_study(\n",
    "#         direction=\"maximize\",\n",
    "#         pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    "#     )\n",
    "#     study.optimize(objective, n_trials=100)\n",
    "\n",
    "#     print(\"Best trial:\")\n",
    "#     best = study.best_trial\n",
    "#     print(f\"  Value: {best.value:.4f}\")\n",
    "#     print(\"  Params:\")\n",
    "#     for key, val in best.params.items():\n",
    "#         print(f\"    {key}: {val}\")\n",
    "\n",
    "#     # Optionally, save best hyperparameters to disk\n",
    "#     import json\n",
    "#     with open(\"best_params.json\", \"w\") as f:\n",
    "#         json.dump(best.params, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c3e8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import copy\n",
    "\n",
    "# # Parameters\n",
    "# max_epochs = 4\n",
    "# keep_frac  = 0.5  # keep top 50% after each epoch\n",
    "\n",
    "# # Start with full grid\n",
    "# current_configs = copy.deepcopy(grid_configs)\n",
    "# config_states   = {}  # to optionally resume from saved state dicts\n",
    "\n",
    "# for epoch in range(1, max_epochs + 1):\n",
    "#     print(f\"\\n=== Epoch {epoch}/{max_epochs}: {len(current_configs)} configs ===\")\n",
    "#     epoch_results = []\n",
    "\n",
    "#     for idx, config in enumerate(current_configs, start=1):\n",
    "#         print(f\"  [{idx}/{len(current_configs)}] Config: {config}\")\n",
    "\n",
    "#         # Build model (fresh each epoch or resume)\n",
    "#         model = SpeakerCountCNN(\n",
    "#             input_height=input_height,\n",
    "#             input_width=input_width,\n",
    "#             conv1_out=config[\"conv1_out\"],\n",
    "#             conv2_out=config[\"conv2_out\"],\n",
    "#             conv3_out=config[\"conv3_out\"],\n",
    "#             fc_hidden=config[\"fc_hidden\"],\n",
    "#             dropout_prob=config[\"dropout\"]\n",
    "#         ).to(device)\n",
    "\n",
    "#         # If you’ve stored a state from the previous epoch, load it\n",
    "#         key = tuple(sorted(config.items()))\n",
    "#         if key in config_states:\n",
    "#             model.load_state_dict(config_states[key])\n",
    "\n",
    "#         # Optimizer & loss\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#         # Train exactly one epoch\n",
    "#         train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "#         # Validate\n",
    "#         acc = validate(model, val_loader, device)\n",
    "#         print(f\"    → Val Acc: {acc:.2%}\")\n",
    "\n",
    "#         # Record and save state for next round\n",
    "#         epoch_results.append((config, acc, model.state_dict()))\n",
    "\n",
    "#     # Sort by descending accuracy and keep top fraction\n",
    "#     epoch_results.sort(key=lambda x: x[1], reverse=True)\n",
    "#     keep_n = max(1, math.floor(len(epoch_results) * keep_frac))\n",
    "#     survivors = epoch_results[:keep_n]\n",
    "\n",
    "#     # Prepare for next epoch\n",
    "#     current_configs = [cfg for cfg, _, _ in survivors]\n",
    "#     config_states   = { tuple(sorted(cfg.items())): state\n",
    "#                         for cfg, _, state in survivors }\n",
    "\n",
    "# # After all epochs, the top survivor is:\n",
    "# best_cfg, best_acc, best_state = survivors[0]\n",
    "# print(f\"\\n=== Best Config ===\\nAccuracy: {best_acc:.4f}\\nParams: {best_cfg}\")\n",
    "\n",
    "# # Optionally, save the best model weights:\n",
    "# best_model = SpeakerCountCNN(\n",
    "#     input_height=input_height,\n",
    "#     input_width=input_width,\n",
    "#     conv1_out=best_cfg[\"conv1_out\"],\n",
    "#     conv2_out=best_cfg[\"conv2_out\"],\n",
    "#     conv3_out=best_cfg[\"conv3_out\"],\n",
    "#     fc_hidden=best_cfg[\"fc_hidden\"],\n",
    "#     dropout_prob=best_cfg[\"dropout\"]\n",
    "# ).to(device)\n",
    "# best_model.load_state_dict(best_state)\n",
    "# torch.save(best_model.state_dict(), \"best_GridModel.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1776c",
   "metadata": {},
   "source": [
    "### Final model longer training after grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18fd9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creates torch dataset using spectrogram files as 'x' and csv of labels as 'y'\n",
    "# dataset = SpectrogramDataset(csv_file=r\"data/spectrogram_labels.csv\", data_dir=r\"data/spectrograms\") # grab dataset and convert to tensor\n",
    "\n",
    "\n",
    "# # fixed image size\n",
    "# input_height = 96\n",
    "# input_width = 64\n",
    "\n",
    "\n",
    "# # manual train/test/split\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "# # batch loader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "# # cpu or gpu; whichever is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# # basic loss and optimiser\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "# model = best_model\n",
    "\n",
    "\n",
    "\n",
    "# # Model training\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     start_time = time()\n",
    "\n",
    "#     # tqdm shows a progress bar\n",
    "#     for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(x)\n",
    "#         loss = criterion(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     end_time = time()\n",
    "#     epoch_duration = end_time - start_time\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} — Loss: {avg_loss:.4f} — Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "# # Validation - this is the important metric\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for x, y in val_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         output = model(x)\n",
    "#         preds = output.argmax(dim=1)\n",
    "#         correct += (preds == y).sum().item()\n",
    "#         total += y.size(0)\n",
    "\n",
    "# print(f\"Validation Accuracy: {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6429b37",
   "metadata": {},
   "source": [
    "## Vanishing gradient\n",
    "\n",
    "The setup above caused vanishing gradient. Trying skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60cb5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # 1) Residual block with skip connection\n",
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch, stride=1):\n",
    "#         super().__init__()\n",
    "#         self.conv_block = nn.Sequential(\n",
    "#             nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_ch),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_ch)\n",
    "#         )\n",
    "#         # projection if we change channels or downsample\n",
    "#         self.proj = nn.Sequential()\n",
    "#         if stride != 1 or in_ch != out_ch:\n",
    "#             self.proj = nn.Sequential(\n",
    "#                 nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(out_ch)\n",
    "#             )\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = self.proj(x)\n",
    "#         out = self.conv_block(x)\n",
    "#         out += identity\n",
    "#         return self.relu(out)\n",
    "\n",
    "\n",
    "\n",
    "# # 2) Updated SpeakerCountCNN using ResidualBlock\n",
    "# class SpeakerCountCNN_skip(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         conv1_out,\n",
    "#         conv2_out,\n",
    "#         conv3_out,\n",
    "#         fc_hidden,\n",
    "#         dropout_prob,\n",
    "#         input_height=input_height,\n",
    "#         input_width=input_width\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         # replace conv+bn+relu with residual blocks\n",
    "#         self.layer1 = ResidualBlock(1, conv1_out, stride=1)\n",
    "#         self.layer2 = ResidualBlock(conv1_out, conv2_out, stride=1)\n",
    "#         self.layer3 = ResidualBlock(conv2_out, conv3_out, stride=1)\n",
    "\n",
    "#         self.pool    = nn.MaxPool2d(2, 2)\n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "#         # compute flattened size dynamically\n",
    "#         with torch.no_grad():\n",
    "#             dummy = torch.zeros(1, 1, input_height, input_width)\n",
    "#             x = self.pool(self.layer1(dummy))\n",
    "#             x = self.pool(self.layer2(x))\n",
    "#             x = self.pool(self.layer3(x))\n",
    "#             flattened_size = x.view(1, -1).shape[1]\n",
    "\n",
    "#         self.fc1 = nn.Linear(flattened_size, fc_hidden)\n",
    "#         self.fc2 = nn.Linear(fc_hidden, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(self.layer1(x))\n",
    "#         x = self.pool(self.layer2(x))\n",
    "#         x = self.pool(self.layer3(x))\n",
    "\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.dropout(x)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30633e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# config = {'lr': 0.0005, 'dropout': 0.35, 'fc_hidden': 64, 'conv1_out': 32, 'conv2_out': 16, 'conv3_out': 128, 'optimizer': 'Adam'}\n",
    "\n",
    "# model = SpeakerCountCNN(\n",
    "#     input_height = input_height,\n",
    "#     input_width  = input_width,\n",
    "#     conv1_out    = config[\"conv1_out\"],\n",
    "#     conv2_out    = config[\"conv2_out\"],\n",
    "#     conv3_out    = config[\"conv3_out\"],\n",
    "#     fc_hidden    = config[\"fc_hidden\"],\n",
    "#     dropout_prob = config[\"dropout\"]\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# # Prepare dataset and simple split\n",
    "# dataset = SpectrogramDataset(\n",
    "#     csv_file = r\"data/spectrogram_labels.csv\",\n",
    "#     data_dir  = r\"data/spectrograms\"\n",
    "# )\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size   = len(dataset) - train_size\n",
    "# train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# # Loss, optimizer, scheduler\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "\n",
    "\n",
    "# # Training loop with scheduler.step AFTER avg_loss is computed\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     start = time()\n",
    "\n",
    "#     for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         out  = model(x)\n",
    "#         loss = criterion(out, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     scheduler.step(avg_loss)               # ← here, after avg_loss\n",
    "#     duration = time() - start\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} — Loss: {avg_loss:.4f} — Time: {duration:.1f}s\")\n",
    "\n",
    "\n",
    "\n",
    "# # Validation\n",
    "# model.eval()\n",
    "# correct, total = 0, 0\n",
    "# with torch.no_grad():\n",
    "#     for x, y in val_loader:\n",
    "#         x, y    = x.to(device), y.to(device)\n",
    "#         preds   = model(x).argmax(dim=1)\n",
    "#         correct += (preds == y).sum().item()\n",
    "#         total   += y.size(0)\n",
    "\n",
    "# print(f\"Validation Accuracy: {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4350eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# y_true, y_pred = [], []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for x, y in val_loader:\n",
    "#         x = x.to(device)\n",
    "#         preds = model(x).argmax(dim=1).cpu()\n",
    "#         y_true.extend(y.numpy())\n",
    "#         y_pred.extend(preds.numpy())\n",
    "\n",
    "\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "# disp.plot(cmap='Blues', values_format='d')\n",
    "# plt.title(\"Confusion Matrix — Validation\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef2d760",
   "metadata": {},
   "source": [
    "### Save when happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b52e366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# torch.save(model.state_dict(), 'SpeakerCountCNN_v0.11.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efdfbe",
   "metadata": {},
   "source": [
    "#### Init & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ff5dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - packages & helpers\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Data loaders for full data set\n",
    "def get_loaders(dataset, train_frac=0.8, bs=32, seed=42):\n",
    "    n = len(dataset)\n",
    "    t = int(train_frac * n)\n",
    "    train_ds, val_ds = random_split(dataset, [t, n-t], generator=torch.Generator().manual_seed(seed))\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "            DataLoader(val_ds, batch_size=bs, shuffle=False))\n",
    "\n",
    "\n",
    "\n",
    "# Data loaders for subset, makes hp tuning quicker (stratified sampling prevents class imbalance)\n",
    "def get_stratified_loaders(\n",
    "    dataset,\n",
    "    subset_frac: float = 0.3,\n",
    "    train_frac: float = 0.8,\n",
    "    batch_size: int = 32,\n",
    "    random_state: int = 42,\n",
    "    shuffle_train: bool = True\n",
    "):\n",
    "\n",
    "    # 1. Stratified sampling of the full dataset\n",
    "    full_indices = list(range(len(dataset)))\n",
    "    labels = dataset.labels\n",
    "    subset_idx, _ = train_test_split(\n",
    "        full_indices,\n",
    "        train_size=subset_frac,\n",
    "        stratify=labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    reduced_ds = Subset(dataset, subset_idx)\n",
    "\n",
    "    # 2. Stratified train/val split of the reduced dataset\n",
    "    reduced_labels = [labels[i] for i in subset_idx]\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(reduced_ds))),\n",
    "        train_size=train_frac,\n",
    "        stratify=reduced_labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 3. Build DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        Subset(reduced_ds, train_idx),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_train\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        Subset(reduced_ds, val_idx),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n",
    "# Model builder\n",
    "def build_model(cfg, input_h, input_w):\n",
    "    return SpeakerCountCNN(\n",
    "        input_height=input_h, input_width=input_w,\n",
    "        conv1_out=cfg['conv1_out'], conv2_out=cfg['conv2_out'],\n",
    "        conv3_out=cfg['conv3_out'], fc_hidden=cfg['fc_hidden'],\n",
    "        dropout_prob=cfg['dropout']\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# One‐epoch training with tqdm\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "def train_one_epoch(model, loader, opt, crit, device):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for x, y in tqdm(loader, desc=\"Training\", unit=\"batch\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = crit(model(x), y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    return total / len(loader)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds==y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430fdfd",
   "metadata": {},
   "source": [
    "#### Define the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "401129cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpectrogramDataset(csv_file=r\"data/spectrogram_labels.csv\", data_dir=r\"data/spectrograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e52df5",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning with Successive‐Halving (SH) via Optuna\n",
    "SH runs all models over one epoch and keeps models that perform above a pruning threshold. This repeats, reducing the model set each epoch, until a winner is clear.\n",
    "\n",
    "The Optuna package was chosen because it caches model weights each epoch, hugely reducing compute. Additionally it uses continuous, asynchronous pruning rather than fixed-round halving, improving scalability and resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d8683e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-26 15:55:38,101] A new study created in memory with name: 0-2_Speaker_HPTune\n",
      "[I 2025-07-26 16:01:49,488] Trial 0 finished with value: 0.8347145488029466 and parameters: {'lr': 0.001, 'dropout': 0.25, 'fc_hidden': 128, 'conv1_out': 16, 'conv2_out': 16, 'conv3_out': 64}. Best is trial 0 with value: 0.8347145488029466.\n",
      "[I 2025-07-26 16:05:11,727] Trial 1 finished with value: 0.8416206261510129 and parameters: {'lr': 0.001, 'dropout': 0.3, 'fc_hidden': 128, 'conv1_out': 32, 'conv2_out': 16, 'conv3_out': 64}. Best is trial 1 with value: 0.8416206261510129.\n",
      "[I 2025-07-26 16:12:56,032] Trial 2 pruned. \n",
      "[I 2025-07-26 16:18:56,443] Trial 3 pruned. \n",
      "[I 2025-07-26 16:21:39,525] Trial 4 pruned. \n",
      "[I 2025-07-26 16:23:56,875] Trial 5 pruned. \n",
      "[I 2025-07-26 16:32:18,383] Trial 6 pruned. \n",
      "[I 2025-07-26 16:33:22,392] Trial 7 pruned. \n",
      "[I 2025-07-26 16:38:37,751] Trial 8 pruned. \n",
      "[I 2025-07-26 16:43:26,702] Trial 9 pruned. \n",
      "[I 2025-07-26 16:50:09,848] Trial 10 pruned. \n",
      "[I 2025-07-26 16:52:58,996] Trial 11 pruned. \n",
      "[I 2025-07-26 16:54:35,389] Trial 12 pruned. \n",
      "[I 2025-07-26 17:01:34,288] Trial 13 pruned. \n",
      "[I 2025-07-26 17:05:42,904] Trial 14 pruned. \n",
      "[I 2025-07-26 17:09:08,128] Trial 15 pruned. \n",
      "[I 2025-07-26 17:17:12,076] Trial 16 pruned. \n",
      "[I 2025-07-26 17:22:49,633] Trial 17 pruned. \n",
      "[I 2025-07-26 17:28:14,851] Trial 18 pruned. \n",
      "[I 2025-07-26 17:33:42,963] Trial 19 pruned. \n",
      "[I 2025-07-26 17:47:31,181] Trial 20 finished with value: 0.8379373848987108 and parameters: {'lr': 0.001, 'dropout': 0.25, 'fc_hidden': 128, 'conv1_out': 32, 'conv2_out': 16, 'conv3_out': 64}. Best is trial 1 with value: 0.8416206261510129.\n",
      "[I 2025-07-26 17:52:02,333] Trial 21 pruned. \n",
      "[I 2025-07-26 18:03:41,949] Trial 22 pruned. \n",
      "[I 2025-07-26 18:18:37,783] Trial 23 pruned. \n",
      "[I 2025-07-26 18:20:59,020] Trial 24 pruned. \n",
      "[I 2025-07-26 18:22:59,291] Trial 25 pruned. \n",
      "[I 2025-07-26 18:25:49,836] Trial 26 pruned. \n",
      "[I 2025-07-26 18:27:56,008] Trial 27 pruned. \n",
      "[I 2025-07-26 18:31:05,197] Trial 28 pruned. \n",
      "[I 2025-07-26 18:35:12,431] Trial 29 pruned. \n",
      "[I 2025-07-26 18:43:04,574] Trial 30 pruned. \n",
      "[I 2025-07-26 18:50:40,983] Trial 31 pruned. \n",
      "[I 2025-07-26 18:57:39,664] Trial 32 pruned. \n",
      "[I 2025-07-26 19:10:07,109] Trial 33 pruned. \n",
      "[I 2025-07-26 19:21:56,184] Trial 34 pruned. \n",
      "[I 2025-07-26 19:26:57,352] Trial 35 pruned. \n",
      "[I 2025-07-26 19:36:06,835] Trial 36 pruned. \n",
      "[I 2025-07-26 19:40:50,141] Trial 37 pruned. \n",
      "[I 2025-07-26 19:52:50,760] Trial 38 finished with value: 0.8406998158379374 and parameters: {'lr': 0.001, 'dropout': 0.35, 'fc_hidden': 64, 'conv1_out': 32, 'conv2_out': 64, 'conv3_out': 64}. Best is trial 1 with value: 0.8416206261510129.\n",
      "[I 2025-07-26 19:59:03,922] Trial 39 pruned. \n",
      "[I 2025-07-26 20:10:54,356] Trial 40 pruned. \n",
      "[I 2025-07-26 20:18:38,173] Trial 41 pruned. \n",
      "[I 2025-07-26 20:23:20,038] Trial 42 pruned. \n",
      "[I 2025-07-26 20:30:11,734] Trial 43 pruned. \n",
      "[I 2025-07-26 20:36:06,204] Trial 44 pruned. \n",
      "[I 2025-07-26 20:47:25,186] Trial 45 pruned. \n",
      "[I 2025-07-26 20:55:39,017] Trial 46 pruned. \n",
      "[I 2025-07-26 21:01:36,233] Trial 47 pruned. \n",
      "[I 2025-07-26 21:04:46,164] Trial 48 pruned. \n",
      "[I 2025-07-26 21:17:13,061] Trial 49 finished with value: 0.8522099447513812 and parameters: {'lr': 0.0005, 'dropout': 0.3, 'fc_hidden': 256, 'conv1_out': 16, 'conv2_out': 64, 'conv3_out': 64}. Best is trial 49 with value: 0.8522099447513812.\n",
      "[I 2025-07-26 21:21:00,410] Trial 50 pruned. \n",
      "[I 2025-07-26 21:24:41,507] Trial 51 pruned. \n",
      "[I 2025-07-26 21:37:06,260] Trial 52 finished with value: 0.8420810313075506 and parameters: {'lr': 0.0005, 'dropout': 0.3, 'fc_hidden': 256, 'conv1_out': 16, 'conv2_out': 64, 'conv3_out': 64}. Best is trial 49 with value: 0.8522099447513812.\n",
      "[I 2025-07-26 21:40:47,645] Trial 53 pruned. \n",
      "[I 2025-07-26 21:53:10,853] Trial 54 pruned. \n",
      "[I 2025-07-26 21:56:58,083] Trial 55 pruned. \n",
      "[I 2025-07-26 22:03:55,119] Trial 56 pruned. \n",
      "[I 2025-07-26 22:07:41,081] Trial 57 pruned. \n",
      "[I 2025-07-26 22:13:14,172] Trial 58 pruned. \n",
      "[I 2025-07-26 22:20:13,863] Trial 59 finished with value: 0.8416206261510129 and parameters: {'lr': 0.0005, 'dropout': 0.3, 'fc_hidden': 256, 'conv1_out': 16, 'conv2_out': 64, 'conv3_out': 64}. Best is trial 49 with value: 0.8522099447513812.\n",
      "[I 2025-07-26 22:21:44,527] Trial 60 pruned. \n",
      "[I 2025-07-26 22:33:57,564] Trial 61 finished with value: 0.8434622467771639 and parameters: {'lr': 0.0005, 'dropout': 0.3, 'fc_hidden': 256, 'conv1_out': 16, 'conv2_out': 64, 'conv3_out': 64}. Best is trial 49 with value: 0.8522099447513812.\n",
      "[I 2025-07-26 22:37:36,030] Trial 62 pruned. \n",
      "[I 2025-07-26 22:41:17,896] Trial 63 pruned. \n",
      "[I 2025-07-26 22:44:58,298] Trial 64 pruned. \n",
      "[I 2025-07-26 22:54:04,563] Trial 65 pruned. \n",
      "[I 2025-07-26 22:58:43,866] Trial 66 pruned. \n",
      "[I 2025-07-26 23:03:57,181] Trial 67 pruned. \n",
      "[I 2025-07-26 23:14:50,310] Trial 68 pruned. \n",
      "[I 2025-07-26 23:20:06,794] Trial 69 pruned. \n",
      "[I 2025-07-26 23:23:26,379] Trial 70 pruned. \n",
      "[I 2025-07-26 23:25:42,962] Trial 71 pruned. \n",
      "[I 2025-07-26 23:33:41,768] Trial 72 pruned. \n",
      "[I 2025-07-26 23:37:08,268] Trial 73 pruned. \n",
      "[I 2025-07-26 23:38:47,817] Trial 74 pruned. \n",
      "[I 2025-07-26 23:39:49,426] Trial 75 pruned. \n",
      "[I 2025-07-26 23:49:15,929] Trial 76 pruned. \n",
      "[I 2025-07-26 23:51:50,470] Trial 77 pruned. \n",
      "[I 2025-07-26 23:57:09,221] Trial 78 pruned. \n",
      "[I 2025-07-27 00:03:16,647] Trial 79 pruned. \n",
      "[I 2025-07-27 00:10:04,282] Trial 80 pruned. \n",
      "[I 2025-07-27 00:15:22,466] Trial 81 pruned. \n",
      "[I 2025-07-27 00:19:25,487] Trial 82 pruned. \n",
      "[I 2025-07-27 00:23:42,348] Trial 83 pruned. \n",
      "[I 2025-07-27 00:27:46,995] Trial 84 pruned. \n",
      "[I 2025-07-27 00:34:01,299] Trial 85 pruned. \n",
      "[I 2025-07-27 00:39:16,280] Trial 86 pruned. \n",
      "[I 2025-07-27 00:52:14,903] Trial 87 pruned. \n",
      "[I 2025-07-27 01:08:31,270] Trial 88 pruned. \n",
      "[I 2025-07-27 01:16:02,220] Trial 89 pruned. \n",
      "[I 2025-07-27 01:33:15,307] Trial 90 finished with value: 0.8439226519337016 and parameters: {'lr': 0.0005, 'dropout': 0.3, 'fc_hidden': 256, 'conv1_out': 32, 'conv2_out': 16, 'conv3_out': 64}. Best is trial 49 with value: 0.8522099447513812.\n",
      "[I 2025-07-27 01:47:50,935] Trial 91 pruned. \n",
      "[I 2025-07-27 02:01:03,062] Trial 92 pruned. \n",
      "[I 2025-07-27 02:06:19,730] Trial 93 pruned. \n",
      "[I 2025-07-27 02:11:49,769] Trial 94 pruned. \n",
      "[I 2025-07-27 02:17:38,751] Trial 95 pruned. \n",
      "[I 2025-07-27 02:25:26,786] Trial 96 pruned. \n",
      "[I 2025-07-27 02:42:16,743] Trial 97 pruned. \n",
      "[I 2025-07-27 02:47:37,155] Trial 98 pruned. \n",
      "[I 2025-07-27 02:55:36,498] Trial 99 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HPs: {'lr': 0.0005, 'dropout': 0.3, 'fc_hidden': 256, 'conv1_out': 16, 'conv2_out': 64, 'conv3_out': 64}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# SH trial over 4 epochs; after 4 epochs the best model is chosen from remaining models.\n",
    "def objective(trial):\n",
    "\n",
    "    cfg = {\n",
    "      'lr': trial.suggest_categorical('lr', [5e-4,1e-3,5e-3]),\n",
    "      'dropout': trial.suggest_categorical('dropout', [0.25,0.3,0.35]),\n",
    "      'fc_hidden': trial.suggest_categorical('fc_hidden',[64,128,256]),\n",
    "      'conv1_out': trial.suggest_categorical('conv1_out',[16,32]),\n",
    "      'conv2_out': trial.suggest_categorical('conv2_out',[16,32,64]),\n",
    "      'conv3_out': trial.suggest_categorical('conv3_out',[64,128]),\n",
    "    }\n",
    "\n",
    "    # Helper to build model, see setup\n",
    "    model = build_model(cfg, input_h=96, input_w=64).to(device)\n",
    "\n",
    "    # Adam optim\n",
    "    opt   = optim.Adam(model.parameters(), lr=cfg['lr'])\n",
    "\n",
    "    # Cross entropy objective\n",
    "    crit  = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and comparing to existing models\n",
    "    for epoch in range(5):\n",
    "        train_one_epoch(model, train_loader, opt, crit, device)\n",
    "        val_acc = validate(model, val_loader, device)\n",
    "        trial.report(val_acc, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "# Creates a stratified subset, makes hp tuning quicker\n",
    "train_loader, val_loader = get_stratified_loaders(\n",
    "    dataset,\n",
    "    subset_frac=0.3,   \n",
    "    train_frac=0.8,    \n",
    "    batch_size=32,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "study  = optuna.create_study(direction='maximize',\n",
    "            pruner=optuna.pruners.SuccessiveHalvingPruner(), study_name='0-2_Speaker_HPTune')\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_cfg = study.best_trial.params\n",
    "print(\"Best HPs:\", best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b3287ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8522099447513812"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439979cb",
   "metadata": {},
   "source": [
    "### Final Model training, based on best config found above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56f42387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [13:47<00:00,  1.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Loss 0.5027 — 827.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [16:31<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Loss 0.4432 — 991.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [13:11<00:00,  1.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Loss 0.4149 — 791.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [13:15<00:00,  1.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Loss 0.3926 — 795.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [11:50<00:00,  1.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Loss 0.3790 — 710.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [06:43<00:00,  2.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Loss 0.3681 — 403.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [11:45<00:00,  1.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 — Loss 0.3542 — 705.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [09:32<00:00,  1.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 — Loss 0.3434 — 572.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [10:31<00:00,  1.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 — Loss 0.3282 — 631.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 905/905 [07:19<00:00,  2.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 — Loss 0.3114 — 439.3s\n",
      "Final val accuracy: 86.93%\n"
     ]
    }
   ],
   "source": [
    "# Model with config from Optuna study\n",
    "final_model = build_model(best_cfg, 96, 64).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "opt = optim.Adam(final_model.parameters(), lr=best_cfg['lr'])\n",
    "\n",
    "\n",
    "\n",
    "# Cross Entropy loss\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "# Loaders\n",
    "train_loader, val_loader = get_loaders(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Train\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time()\n",
    "    loss = train_one_epoch(final_model, train_loader, opt, crit, device)  # tqdm is inside this\n",
    "    dur = time() - t0\n",
    "    print(f\"Epoch {epoch+1} — Loss {loss:.4f} — {dur:.1f}s\")\n",
    "\n",
    "\n",
    "\n",
    "# Validate\n",
    "val_acc = validate(final_model, val_loader, device)\n",
    "print(f\"Final val accuracy: {val_acc:.2%}\")\n",
    "torch.save(final_model.state_dict(), r\"src/model/best_0-2_3layer.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc99260",
   "metadata": {},
   "source": [
    "### Confusion Matrix (CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        preds = model(x).argmax(dim=1).cpu()\n",
    "        y_true.extend(y.numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix — Validation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
