@inproceedings{pyannote2020,
  Title = {{pyannote.audio: neural building blocks for speaker diarization}},
  Author = {{Bredin}, Herv{\'e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},
  Booktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},
  Address = {Barcelona, Spain},
  Month = {May},
  Year = {2020},
}


@inproceedings{pyannote2021,
  Title = {{End-to-end speaker segmentation for overlap-aware resegmentation}},
  Author = {{Bredin}, Herv{\'e} and {Laurent}, Antoine},
  Booktitle = {Proc. Interspeech 2021},
  Address = {Brno, Czech Republic},
  Month = {August},
  Year = {2021},
}


@inproceedings{Ng_2021, series={interspeech_2021},
   title={Pushing the Limits of Non-Autoregressive Speech Recognition},
   url={http://dx.doi.org/10.21437/Interspeech.2021-337},
   DOI={10.21437/interspeech.2021-337},
   booktitle={Interspeech 2021},
   publisher={ISCA},
   author={Ng, Edwin G. and Chiu, Chung-Cheng and Zhang, Yu and Chan, William},
   year={2021},
   month=aug, pages={3725–3729},
   collection={interspeech_2021} }


@inproceedings{Xu_2024,
  author    = {Xu, Anfeng and Huang, Kevin and Feng, Tiantian and Shen, Lue and Tager-Flusberg, Helen and Narayanan, Shrikanth},
  title     = {Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions},
  booktitle = {Proceedings of Interspeech 2024},
  pages     = {5193--5197},
  year      = {2024},
  doi       = {10.21437/Interspeech.2024-717}
}



@inproceedings{Song_2021,
author = {Song, Yuanfeng and Jiang, Di and Zhao, Xuefang and Huang, Xiaoling and Xu, Qian and Wong, Raymond Chi-Wing and Yang, Qiang},
title = {SmartMeeting: Automatic Meeting Transcription and Summarization for In-Person Conversations},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3478556},
doi = {10.1145/3474085.3478556},
abstract = {Meetings are a necessary part of the operations of any institution, whether they are held online or in-person. However, meeting transcription and summarization are always painful requirements since they involve tedious human effort. This drives the need for automatic meeting transcription and summarization (AMTS) systems. A successful AMTS system relies on systematic integration of multiple natural language processing (NLP) techniques, such as automatic speech recognition, speaker identification, and meeting summarization, which are traditionally developed separately and validated offline with standard datasets. In this demonstration, we provide a novel productive meeting tool named SmartMeeting, which enables users to automatically record, transcribe, summarize, and manage the information in an in-person meeting. SmartMeeting transcribes every word on the fly, enriches the transcript with speaker identification and voice separation, and extracts essential decisions and crucial insights automatically. In our demonstration, the audience can experience the great potential of the state-of-the-art NLP techniques in this real-life application.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2777–2779},
numpages = {3},
keywords = {speaker verification, meeting transcription, meeting summarization},
location = {Virtual Event, China},
series = {MM '21}
}


@article{Crocco_2016,
author = {Crocco, Marco and Cristani, Marco and Trucco, Andrea and Murino, Vittorio},
title = {Audio Surveillance: A Systematic Review},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2871183},
doi = {10.1145/2871183},
abstract = {Despite surveillance systems becoming increasingly ubiquitous in our living environment, automated surveillance, currently based on video sensory modality and machine intelligence, lacks most of the time the robustness and reliability required in several real applications. To tackle this issue, audio sensory devices have been incorporated, both alone or in combination with video, giving birth in the past decade, to a considerable amount of research. In this article, audio-based automated surveillance methods are organized into a comprehensive survey: A general taxonomy, inspired by the more widespread video surveillance field, is proposed to systematically describe the methods covering background subtraction, event classification, object tracking, and situation analysis. For each of these tasks, all the significant works are reviewed, detailing their pros and cons and the context for which they have been proposed. Moreover, a specific section is devoted to audio features, discussing their expressiveness and their employment in the above-described tasks. Differing from other surveys on audio processing and analysis, the present one is specifically targeted to automated surveillance, highlighting the target applications of each described method and providing the reader with a systematic and schematic view useful for retrieving the most suited algorithms for each specific requirement.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {52},
numpages = {46},
keywords = {multimodal surveillance, audio surveillance, Automated surveillance}
}


@book{Helander_2014,
  title={Handbook of human-computer interaction},
  author={Helander, Martin G},
  year={2014},
  publisher={Elsevier}
}


@article{yella2014,
  title={Overlapping speech detection using long-term conversational features for speaker diarization in meeting room conversations},
  author={Yella, Sree Harsha and Bourlard, Herv{\'e}},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={22},
  number={12},
  pages={1688--1700},
  year={2014},
  publisher={IEEE}
}