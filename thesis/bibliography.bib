@inproceedings{pyannote2020,
  Title = {{pyannote.audio: neural building blocks for speaker diarization}},
  Author = {{Bredin}, Herv{\'e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},
  Booktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},
  Address = {Barcelona, Spain},
  Month = {May},
  Year = {2020},
}



@inproceedings{pyannote2021,
  Title = {{End-to-end speaker segmentation for overlap-aware resegmentation}},
  Author = {{Bredin}, Herv{\'e} and {Laurent}, Antoine},
  Booktitle = {Proc. Interspeech 2021},
  Address = {Brno, Czech Republic},
  Month = {August},
  Year = {2021},
}



@misc{resnet50_researchgate,
  title        = {Improvement of emotion recognition from facial images using deep learning and early stopping cross validation - Scientific Figure on ResearchGate},
  howpublished = {\url{https://www.researchgate.net/figure/a-Architecture-of-the-ResNet50-model-b-Residual-block-3-layer-bottleneck-block-the_fig10_359733554}},
  note         = {[Accessed 18 Aug 2025]},
  year         = {2022}
}



@inproceedings{multidecoder_dprnn,
  title={Multi-Decoder DPRNN: High Accuracy Source Counting and Separation},
  author={Zhu, Junzhe and Yeh, Raymond A. and Hasegawa-Johnson, Mark},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3420--3424},
  year={2021},
  doi={10.1109/ICASSP39728.2021.9414205},
  url={https://arxiv.org/abs/2011.12022}
}


@misc{openAIwhisper,
  author = {Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  year = {2022},
  url = {https://cdn.openai.com/papers/whisper.pdf},
  note = {Accessed: 2025-08-18}
}

@inproceedings{cosine_similarity,
  title={Unsupervised speaker adaptation based on the cosine similarity for text-independent speaker verification.},
  author={Shum, Stephen and Dehak, Najim and Dehak, Reda and Glass, James R},
  booktitle={Odyssey},
  volume={6},
  pages={16},
  year={2010}
}



@inproceedings{Ng_2021, series={interspeech_2021},
   title={Pushing the Limits of Non-Autoregressive Speech Recognition},
   url={http://dx.doi.org/10.21437/Interspeech.2021-337},
   DOI={10.21437/interspeech.2021-337},
   booktitle={Interspeech 2021},
   publisher={ISCA},
   author={Ng, Edwin G. and Chiu, Chung-Cheng and Zhang, Yu and Chan, William},
   year={2021},
   month=aug, pages={3725–3729},
   collection={interspeech_2021} }


@inproceedings{Xu_2024,
  author    = {Xu, Anfeng and Huang, Kevin and Feng, Tiantian and Shen, Lue and Tager-Flusberg, Helen and Narayanan, Shrikanth},
  title     = {Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions},
  booktitle = {Proceedings of Interspeech 2024},
  pages     = {5193--5197},
  year      = {2024},
  doi       = {10.21437/Interspeech.2024-717}
}



@inproceedings{Song_2021,
author = {Song, Yuanfeng and Jiang, Di and Zhao, Xuefang and Huang, Xiaoling and Xu, Qian and Wong, Raymond Chi-Wing and Yang, Qiang},
title = {SmartMeeting: Automatic Meeting Transcription and Summarization for In-Person Conversations},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3478556},
doi = {10.1145/3474085.3478556},
abstract = {Meetings are a necessary part of the operations of any institution, whether they are held online or in-person. However, meeting transcription and summarization are always painful requirements since they involve tedious human effort. This drives the need for automatic meeting transcription and summarization (AMTS) systems. A successful AMTS system relies on systematic integration of multiple natural language processing (NLP) techniques, such as automatic speech recognition, speaker identification, and meeting summarization, which are traditionally developed separately and validated offline with standard datasets. In this demonstration, we provide a novel productive meeting tool named SmartMeeting, which enables users to automatically record, transcribe, summarize, and manage the information in an in-person meeting. SmartMeeting transcribes every word on the fly, enriches the transcript with speaker identification and voice separation, and extracts essential decisions and crucial insights automatically. In our demonstration, the audience can experience the great potential of the state-of-the-art NLP techniques in this real-life application.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2777–2779},
numpages = {3},
keywords = {speaker verification, meeting transcription, meeting summarization},
location = {Virtual Event, China},
series = {MM '21}
}


@article{Crocco_2016,
author = {Crocco, Marco and Cristani, Marco and Trucco, Andrea and Murino, Vittorio},
title = {Audio Surveillance: A Systematic Review},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2871183},
doi = {10.1145/2871183},
abstract = {Despite surveillance systems becoming increasingly ubiquitous in our living environment, automated surveillance, currently based on video sensory modality and machine intelligence, lacks most of the time the robustness and reliability required in several real applications. To tackle this issue, audio sensory devices have been incorporated, both alone or in combination with video, giving birth in the past decade, to a considerable amount of research. In this article, audio-based automated surveillance methods are organized into a comprehensive survey: A general taxonomy, inspired by the more widespread video surveillance field, is proposed to systematically describe the methods covering background subtraction, event classification, object tracking, and situation analysis. For each of these tasks, all the significant works are reviewed, detailing their pros and cons and the context for which they have been proposed. Moreover, a specific section is devoted to audio features, discussing their expressiveness and their employment in the above-described tasks. Differing from other surveys on audio processing and analysis, the present one is specifically targeted to automated surveillance, highlighting the target applications of each described method and providing the reader with a systematic and schematic view useful for retrieving the most suited algorithms for each specific requirement.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {52},
numpages = {46},
keywords = {multimodal surveillance, audio surveillance, Automated surveillance}
}


@book{Helander_2014,
  title={Handbook of human-computer interaction},
  author={Helander, Martin G},
  year={2014},
  publisher={Elsevier}
}


@article{yella2014,
  title={Overlapping speech detection using long-term conversational features for speaker diarization in meeting room conversations},
  author={Yella, Sree Harsha and Bourlard, Herv{\'e}},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={22},
  number={12},
  pages={1688--1700},
  year={2014},
  publisher={IEEE}
}



@inproceedings{torchaudio,
  title={Torchaudio: Building blocks for audio and speech processing},
  author={Yang, Yao-Yuan and Hira, Moto and Ni, Zhaoheng and Astafurov, Artyom and Chen, Caroline and Puhrsch, Christian and Pollack, David and Genzel, Dmitriy and Greenberg, Donny and Yang, Edward Z and others},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6982--6986},
  year={2022},
  organization={IEEE}
}


@article{sciencedirect,
title = {A review of deep learning techniques for speech processing},
journal = {Information Fusion},
volume = {99},
pages = {101869},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101869},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001859},
author = {Ambuj Mehrish and Navonil Majumder and Rishabh Bharadwaj and Rada Mihalcea and Soujanya Poria},
keywords = {Deep learning, Speech processing, Transformers, Survey, Trends},
abstract = {The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field’s evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field.}
}



@article{earlySpeech1,
	author = {{HATON, J.-P.}},
	title = {Problems and solutions for noisy speech recognition},
	DOI= "10.1051/jp4:1994592",
	url= "https://doi.org/10.1051/jp4:1994592",
	journal = {J. Phys. IV France},
	year = 1994,
	volume = 04,
	number = C5,
	pages = "C5-439-C5-448",
	month = "",
}



@misc{musan2015,
  author = {David Snyder and Guoguo Chen and Daniel Povey},
  title = {{MUSAN}: {A} {M}usic, {S}peech, and {N}oise {C}orpus},
  year = {2015},
  eprint = {1510.08484},
  note = {arXiv:1510.08484v1}
}



@INPROCEEDINGS{RoomImpulseResponseDatabase,
  author={Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Seltzer, Michael L. and Khudanpur, Sanjeev},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A study on data augmentation of reverberant speech for robust speech recognition}, 
  year={2017},
  url={https://www.openslr.org/28},
  pages={5220-5224},
  keywords={Speech;Training data;Acoustics;Databases;Data models;Training;Probability distribution;reverberation;augmentation;deep neural network;room impulse responses},
  doi={10.1109/ICASSP.2017.7953152}}



@inproceedings{ResNet50,
  title={Deep Residual Learning for Image Recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={770--778},
  year={2016}
}


@misc{bat_echolocation_jones,
  author       = {Gareth Jones},
  title        = {Echolocation Calls of British Bats},
  year         = 2019,
  url          = {https://www.garethjoneslab.com/british-bat-echolocation},
  note         = {Accessed: 2025-08-16},
  howpublished = {\url{https://www.garethjoneslab.com/british-bat-echolocation}}
}


@inproceedings{libricss,
  title={{Continuous Speech Separation: Dataset and Analysis}},
  author={Chen, Zhuo and Yoshioka, Takuya and Lu, Liang and Zhou, Tianyan and Meng, Zhong and Luo, Yi and Wu, Jian and Xiao, Xiong and Li, Jinyu},
  booktitle={ICASSP 2020--2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2020},
  organization={IEEE},
  note={arXiv:2001.11482}
}



@article{adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@online{speech_and_harmonics,
  author       = {Arnold, Godfrey Edward},
  title        = {Speech},
  year         = {2024},
  url          = {https://www.britannica.com/topic/speech-language},
  note         = {Accessed: 2025-08-17},
  organization = {Encyclopedia Britannica},
  month        = dec
}


@inproceedings{optuna_2019,
    title={Optuna: A Next-generation Hyperparameter Optimization Framework},
    author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    booktitle={Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
    year={2019}
}


@misc{streamlit,
  title        = {{Streamlit}: A Faster Way to Build and Share Data Apps},
  author       = {{Streamlit}},
  howpublished = {\url{https://streamlit.io/}},
  note         = {Accessed: 2025-08-18}
}


@article{modelsizes,
author = {Zheng, YuYu and Huang, HaoXuan and Chen, Junming},
year = {2024},
month = {02},
pages = {012015},
title = {Comparative analysis of various models for image classification on Cifar-100 dataset},
volume = {2711},
journal = {Journal of Physics: Conference Series},
doi = {10.1088/1742-6596/2711/1/012015}
}

@misc{sounddevice,
  author       = {Michael Brandl},
  title        = {sounddevice: Play and record sound with Python},
  year         = {2021},
  howpublished = {\url{https://python-sounddevice.readthedocs.io/}},
  note         = {Accessed: 2025-08-26}
}