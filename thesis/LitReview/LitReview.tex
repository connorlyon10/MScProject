% !TEX root =  ../Dissertation.tex
\chapter{Literature Review}

\section{Speech Processing}
Speech processing is the field focused on analysing and modelling human vocal communication signals \cite{sciencedirect}. A primary task in speech processing is Automated Speech Recognition (ASR), which aims to convert raw speech audio into text \cite{speechprocessing_review}. ASR consists of many components, each a task in itself; examples include noise cancellation \cite{noisecancellation}, speaker diarisation \cite{diarization} and speaker counting \cite{speaker_counting}. Modern speech processing solutions are often implemented as pipelines for this reason; Pyannote \cite{pyannote2020, pyannote2021}, Speechbrain \cite{speechbrain} and NeMo \cite{nemo} are leading Python packages which use ``modular neural building blocks'' to solve these tasks. Figure~\ref{fig:ideal_pipeline} illustrates one potential structure of such a pipeline.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figs/Intro&Review/FlowChartNew.png}
\caption{A typical speech processing pipeline, making use of SOTA models \& methods \cite{pyannote2020, multidecoder_dprnn,openAIwhisper,cosine_similarity}. The end goal of speech processing is to fully separate and transcribe overlapping audio in a busy environment; each piece of the pipeline attempts a solution of one piece of the problem by assuming that previous parts are solved.}
\label{fig:ideal_pipeline}
\end{figure}


\section{Machine Learning for Speech Processing}
Modern deep learning techniques have revolutionised speech processing, with models achieving state-of-the-art (SOTA) results on large-scale datasets \cite{transformers}. However, it's been understood since early research that successes in controlled or offline settings often do not generalise well to real-world environments \cite{signal_processing_book}.\newline

\noindent Voice activity detection (VAD) and speaker counting are essential upstream components in the speech processing pipeline \cite{importance_of_vad}. VAD identifies whether speech is present in an audio segment, supporting segmentation and reducing computational load for subsequent modules. Speaker counting estimates the number of distinct concurrent speakers, which is critical in overlapped speech scenarios \cite{importance_of_OSD} where diarisation and ASR performance degrade significantly without accurate overlap detection.\newline

\noindent Progress in VAD has been strong, with modern model architectures achieving near perfect performance in controlled conditions \cite{vad_review}. Speaker counting, however, remains challenging due to the complexity of overlapping voices \cite{speaker_counting}. Existing methods use recurrent neural networks for temporal modelling \cite{lstm} and (more recently) transformer based methods \cite{transformers}. Despite these advances, most approaches operate offline and assume access to multi-channel or far-field microphone arrays, limiting their applicability in consumer or embedded contexts \cite{speaker_count_offline}. This is because real-time processing introduces additional constraints: latency, robustness to dynamic overlap, and computational efficiency, all of which degrade as model complexity increases \cite{efficient_ml}. Current systems often struggle to meet these requirements simultaneously. Many models rely on heavy architectures unsuited for deployment on embedded hardware, or are trained on datasets containing multiple microphones which is unrealistic in real-world applications \cite{libricss}. This leaves a gap between academic research and industry deployment.\newline

\noindent In contrast to current SOTA solutions, convolutional neural networks (CNNs) \cite{cnn} provide a promising approach for lightweight, real-time speech processing. CNNs are a deep learning architecture that include convolutional layers. These layers can break down images into abstract features, which are learnable by an artificial neural network \cite{IBM_CNN}. CNNs try to solve classification tasks by defining decision boundaries between classes, and giving an output based on which side of the boundary an input lies on. Unlike recurrent and transformer based architectures, CNNs typically require fewer parameters, as convolution and pooling reduce input dimensionality. This behaviour means that CNNs are well-suited for real-time deployment, especially as edge devices become more popular \cite{edgecomputing}. Initially popularised in computer vision tasks such as image classification and object detection \cite{cnn_other_uses}, CNNs are also well-suited to learning from spectrograms, which represent speech in a two-dimensional time--frequency space \cite{specs_and_cnns}. Spectrograms encode frequency content over time, with intensity representing amplitude (Appendix~\ref{app:bat_spec}). This resembles an image, making them naturally compatible with convolutional feature extractors.\newline



\section{Datasets in Speech Processing}
Several datasets support research in speech processing (Table~\ref{tab:datasets}), including LibriCSS \cite{libricss} which is derived from the very popular LibriSpeech dataset \cite{librispeech} via playing and re-recording in a controlled setting using microphone arrays to simulate realistic room acoustics. Many models exploit multi-channel datasets to maximise spatial information available during training, but this causes major issues as models fail to generalise in a single-channel environment. Dataset size is also important, as large datasets are needed for SOTA models, but overly large datasets are impractical to train in reasonable time.

\begin{table}[H]
\centering
\caption{Summary of key datasets relevant to speech processing.}
\label{tab:datasets}
\begin{tabular}{|l|l|p{8cm}|}
\hline
\textbf{Dataset} & \textbf{Size} & \textbf{Details} \\ \hline
LibriSpeech \cite{librispeech} & 1000h & Widely used ASR benchmark; clean read English speech; single-channel; limited real-world noise. \\ \hline
LibriCSS \cite{libricss} & $\sim$10h & 7-channel multi-speaker recordings derived from LibriSpeech; conversational-style; seminal for overlap and diarisation research. \\ \hline
RIR \& Noise \cite{RoomImpulseResponseDatabase} & Various & Room Impulse Response and noise recordings; used for data augmentation and simulation of acoustic environments. \\ \hline
AMI Meeting Corpus \cite{ami} & 100h & Multi-channel, multi-speaker meeting recordings; natural conversational speech; annotated for diarisation and ASR. \\ \hline
CHiME Challenges (1--6) \cite{chime} & Varies (5--100h) & Series of corpora for robust ASR in noisy, far-field, real-world environments; includes multi-channel recordings. \\ \hline
LibriMix \cite{librimix} & 500h+ & Mixtures of LibriSpeech utterances; designed for source separation and speech enhancement; synthetic overlap. \\ \hline
VoxCeleb (1 \& 2) \cite{voxceleb,voxceleb2}  & 2000h+ & Large-scale speaker recognition dataset from YouTube interviews; diverse conditions; single-channel speech. \\ \hline
\end{tabular}
\end{table}


\subsubsection{Summary}
Voice activity detection and speaker counting are critical tasks in speech processing. While transformer based and recurrent models achieve state-of-the-art performance on large scale datasets, their reliance on multi-channel input and heavy architectures limits applicability in real-time, resource-constrained environments. Existing datasets such as LibriCSS further reinforce this gap by emphasising spatial information that's unavailable to consumer hardware. In contrast, CNN-based approaches offer a lightweight, spectrogram driven alternative that better suits real-world deployment constraints. This clear shortage of datasets and models designed for real-world requirements highlights a clear opportunity for further research.
