% !TEX root =  ../Dissertation.tex
\chapter{Literature Review}
\addcontentsline{toc}{chapter}{Abstract}

\subsection{Speech Processing}
Speech processing is the field focused on analysing and modelling human vocal communication signals \cite{sciencedirect}. A primary task in speech processing is Automated Speech Recognition (ASR), which aims to convert raw speech audio into text \cite{speechprocessing_review}. ASR consists of many components, each a task in itself; examples include noise cancellation \cite{noisecancellation}, speaker diarisation \cite{diarization} and speaker counting \cite{speaker_counting}. Modern speech processing solutions are often implemented as pipelines for this reason; Pyannote \cite{pyannote2020, pyannote2021}, Speechbrain \cite{speechbrain} and NeMo \cite{nemo} are leading Python packages which use ``modular neural building blocks'' to solve these tasks. Figure~\ref{fig:ideal_pipeline} illustrates one potential structure of such a pipeline.

\subsection{Machine Learning for Speech Processing}

Modern deep learning techniques have revolutionised speech processing, with models achieving state-of-the-art results on large-scale datasets \cite{transformers}. However, successes in controlled or offline settings often do not generalise well to noisy, real-world, or resource-constrained environments - this has been understood since early research \cite{signal_processing_book}.\newline\newline Voice activity detection (VAD) and speaker counting are essential upstream components in the speech processing pipeline \cite{importance_of_vad}. VAD identifies whether speech is present in an audio segment, supporting segmentation and reducing computational load for subsequent modules. Speaker counting estimates the number of distinct concurrent speakers, which is critical in overlapped speech scenarios \cite{importance_of_OSD} where diarisation and ASR performance degrade significantly without accurate overlap detection.\newline\newline Progress in VAD has been strong, with modern model architectures achieving near-human performance in controlled conditions \cite{vad_review}. Speaker counting, however, remains challenging due to the complexity of overlapping voices \cite{speaker_counting}. Existing methods use recurrent neural networks for temporal modelling \cite{lstm} and (more recently) transformer-based methods \cite{transformers}. Despite these advances, most approaches operate offline and assume access to multi-channel or far-field microphone arrays, limiting their applicability in consumer or embedded contexts \cite{speaker_count_offline}. This is because real-time processing introduces additional constraints: latency, robustness to dynamic overlap, and computational efficiency, all of which degrade as model complexity increases \cite{efficient_ml}. Current systems often struggle to meet these requirements simultaneously. Many models rely on heavy architectures unsuited for deployment on embedded hardware, or are trained on datasets containing multiple microphones which is unrealistic in real-world applications \cite{libricss}. This leaves a gap between academic research and production-level deployment.

\begin{figure}[H]
\label{fig:ideal_pipeline}
\centering
\centerline{\includegraphics[width=0.8\textwidth]{figs/Intro&Review/FlowChartNew.png}}
\caption{A typical speech processing pipeline, making use of SOTA models \& methods \cite{pyannote2020, multidecoder_dprnn,openAIwhisper,cosine_similarity}. The end-goal of speech processing is to fully seperate and transcribe overlapping audio in a busy environment; each piece of the pipeline attempts a solution of one piece of the problem by assuming that previous parts are solved.}
\label{fig:pipeline}
\end{figure}

In contrast to current state-of-the-art (SOTA) solutions, convolutional neural networks (CNNs) \cite{cnn} provide a promising approach for lightweight, real-time speech processing. CNNs are a deep learning architecture built on convolutional layers; these layers can break down images into abstract features, which are learnable by an artificial neural network \cite{IBM_CNN}. These convolutional layers can be stacked to progressively capture higher-level representations. Unlike recurrent and transformer based architectures, CNNs typically require fewer parameters, as convolution and pooling reduce input dimensionality. This behaviour means that CNNs are well-suited for real-time deployment, especially as edge devices become more popular \cite{edgecomputing}. Initially popularised in computer vision tasks such as image classification and object detection \cite{cnn_other_uses}, CNNs are also well-suited to learning from spectrograms, which represent speech in a two-dimensional time-frequency space \cite{specs_and_cnns}. Spectrograms encode frequency content over time, with intensity representing amplitude (Appendix~\ref{app:bat_spec}). This resembles an image, making them naturally compatible with convolutional feature extractors.\newline



\subsection{Datasets in Speech Processing}
Several datasets support research in speech processing, including LibriSpeech and its derivatives, LibriMix, and LibriCSS \cite{libricss}. LibriCSS is particularly popular; it's derived from LibriSpeech via playing and re-recording in a controlled setting using microphone arrays to simulate realistic room acoustics. Many models exploit multi-channel datasets to maximise spatial information available during training, but this causes major issues as models fail to generalise in a mono-channel environment. This limitation is critical for consumer hardware deployment, which requires compact, low-latency, and robust models. Dataset size is also important: large datasets are needed for SOTA models, but overly large datasets may be impractical to train in reasonable time.

\begin{table}[H]
\centering
\caption{Summary of key datasets relevant to speech processing.}
\label{tab:datasets}
\begin{tabular}{|l|l|p{8cm}|}
\hline
\textbf{Dataset} & \textbf{Size} & \textbf{Details} \\ \hline
LibriSpeech \cite{librispeech} & 1000h & Widely used ASR benchmark; clean read English speech; single-channel; limited real-world noise. \\ \hline
LibriCSS \cite{libricss} & $\sim$10h & 7-channel multi-speaker recordings derived from LibriSpeech; conversational-style; seminal for overlap and diarisation research. \\ \hline
RIR \& Noise \cite{RoomImpulseResponseDatabase} & Various & Room Impulse Response and noise recordings; used for data augmentation and simulation of acoustic environments. \\ \hline
AMI Meeting Corpus \cite{ami} & 100h & Multi-channel, multi-speaker meeting recordings; natural conversational speech; annotated for diarisation and ASR. \\ \hline
CHiME Challenges (1--6) \cite{chime} & Varies (5--100h) & Series of corpora for robust ASR in noisy, far-field, real-world environments; includes multi-channel recordings. \\ \hline
LibriMix \cite{librimix} & 500h+ & Mixtures of LibriSpeech utterances; designed for source separation and speech enhancement; synthetic overlap. \\ \hline
VoxCeleb (1 \& 2) \cite{voxceleb,voxceleb2}  & 2000h+ & Large-scale speaker recognition dataset from YouTube interviews; diverse conditions; single-channel speech. \\ \hline
\end{tabular}
\end{table}


\subsubsection{Summary}
Voice activity detection and speaker counting are critical upstream components of modern speech processing pipelines. While transformer-based and recurrent models achieve state-of-the-art performance on large-scale datasets, their reliance on multi-channel input and heavy architectures limits applicability in real-time, resource-constrained environments. Existing datasets such as LibriCSS further reinforce this gap by emphasising spatial information that's unavailable to consumer hardware. In contrast, CNN-based approaches offer a lightweight, spectrogram-driven alternative that better suits real-world deployment constraints. However, there remains a shortage of both datasets and models designed for the combined requirements of mono-channel input, low-latency inference, and robustness to overlapping speakers, highlighting a clear opportunity for further research.
